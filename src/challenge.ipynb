{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/raw/farmers-protest-tweets-2021-2-4.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession builder and file read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/03 12:00:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/03 12:00:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FarmersProtestTweets\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe print schema and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- conversationId: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- likeCount: long (nullable = true)\n",
      " |-- media: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- duration: double (nullable = true)\n",
      " |    |    |-- fullUrl: string (nullable = true)\n",
      " |    |    |-- previewUrl: string (nullable = true)\n",
      " |    |    |-- thumbnailUrl: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- variants: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |-- contentType: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |-- mentionedUsers: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- created: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- descriptionUrls: string (nullable = true)\n",
      " |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |-- favouritesCount: string (nullable = true)\n",
      " |    |    |-- followersCount: string (nullable = true)\n",
      " |    |    |-- friendsCount: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |-- listedCount: string (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- mediaCount: string (nullable = true)\n",
      " |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |-- protected: string (nullable = true)\n",
      " |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |-- statusesCount: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |    |-- verified: string (nullable = true)\n",
      " |-- outlinks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- quoteCount: long (nullable = true)\n",
      " |-- quotedTweet: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- conversationId: long (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- likeCount: long (nullable = true)\n",
      " |    |-- media: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- duration: double (nullable = true)\n",
      " |    |    |    |-- fullUrl: string (nullable = true)\n",
      " |    |    |    |-- previewUrl: string (nullable = true)\n",
      " |    |    |    |-- thumbnailUrl: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- variants: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |    |-- contentType: string (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- mentionedUsers: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- descriptionUrls: string (nullable = true)\n",
      " |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |-- favouritesCount: string (nullable = true)\n",
      " |    |    |    |-- followersCount: string (nullable = true)\n",
      " |    |    |    |-- friendsCount: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |-- listedCount: string (nullable = true)\n",
      " |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |-- mediaCount: string (nullable = true)\n",
      " |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |-- protected: string (nullable = true)\n",
      " |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |-- statusesCount: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |-- verified: string (nullable = true)\n",
      " |    |-- outlinks: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- quoteCount: long (nullable = true)\n",
      " |    |-- quotedTweet: struct (nullable = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |    |    |-- conversationId: long (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- likeCount: long (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- duration: double (nullable = true)\n",
      " |    |    |    |    |-- fullUrl: string (nullable = true)\n",
      " |    |    |    |    |-- previewUrl: string (nullable = true)\n",
      " |    |    |    |    |-- thumbnailUrl: string (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- variants: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- contentType: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- mentionedUsers: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- descriptionUrls: string (nullable = true)\n",
      " |    |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |    |-- favouritesCount: string (nullable = true)\n",
      " |    |    |    |    |-- followersCount: string (nullable = true)\n",
      " |    |    |    |    |-- friendsCount: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |    |-- listedCount: string (nullable = true)\n",
      " |    |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |    |-- mediaCount: string (nullable = true)\n",
      " |    |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |    |-- protected: string (nullable = true)\n",
      " |    |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |    |-- statusesCount: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |    |-- verified: string (nullable = true)\n",
      " |    |    |-- outlinks: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- quoteCount: long (nullable = true)\n",
      " |    |    |-- quotedTweet: struct (nullable = true)\n",
      " |    |    |    |-- content: string (nullable = true)\n",
      " |    |    |    |-- conversationId: long (nullable = true)\n",
      " |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- lang: string (nullable = true)\n",
      " |    |    |    |-- likeCount: long (nullable = true)\n",
      " |    |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- duration: double (nullable = true)\n",
      " |    |    |    |    |    |-- fullUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- previewUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- thumbnailUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |    |-- variants: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- contentType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- mentionedUsers: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |    |-- descriptionUrls: string (nullable = true)\n",
      " |    |    |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |    |    |-- favouritesCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- followersCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- friendsCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- listedCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |    |    |-- mediaCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- protected: string (nullable = true)\n",
      " |    |    |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |    |    |-- statusesCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |    |    |-- verified: string (nullable = true)\n",
      " |    |    |    |-- outlinks: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- quoteCount: long (nullable = true)\n",
      " |    |    |    |-- quotedTweet: string (nullable = true)\n",
      " |    |    |    |-- renderedContent: string (nullable = true)\n",
      " |    |    |    |-- replyCount: long (nullable = true)\n",
      " |    |    |    |-- retweetCount: long (nullable = true)\n",
      " |    |    |    |-- retweetedTweet: string (nullable = true)\n",
      " |    |    |    |-- source: string (nullable = true)\n",
      " |    |    |    |-- sourceLabel: string (nullable = true)\n",
      " |    |    |    |-- sourceUrl: string (nullable = true)\n",
      " |    |    |    |-- tcooutlinks: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- user: struct (nullable = true)\n",
      " |    |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- descriptionUrls: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |    |-- favouritesCount: long (nullable = true)\n",
      " |    |    |    |    |-- followersCount: long (nullable = true)\n",
      " |    |    |    |    |-- friendsCount: long (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |    |-- listedCount: long (nullable = true)\n",
      " |    |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |    |-- mediaCount: long (nullable = true)\n",
      " |    |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |    |-- statusesCount: long (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |    |-- verified: boolean (nullable = true)\n",
      " |    |    |-- renderedContent: string (nullable = true)\n",
      " |    |    |-- replyCount: long (nullable = true)\n",
      " |    |    |-- retweetCount: long (nullable = true)\n",
      " |    |    |-- retweetedTweet: string (nullable = true)\n",
      " |    |    |-- source: string (nullable = true)\n",
      " |    |    |-- sourceLabel: string (nullable = true)\n",
      " |    |    |-- sourceUrl: string (nullable = true)\n",
      " |    |    |-- tcooutlinks: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user: struct (nullable = true)\n",
      " |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- descriptionUrls: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |    |-- tcourl: string (nullable = true)\n",
      " |    |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |-- favouritesCount: long (nullable = true)\n",
      " |    |    |    |-- followersCount: long (nullable = true)\n",
      " |    |    |    |-- friendsCount: long (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |-- listedCount: long (nullable = true)\n",
      " |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |-- mediaCount: long (nullable = true)\n",
      " |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |-- statusesCount: long (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |-- verified: boolean (nullable = true)\n",
      " |    |-- renderedContent: string (nullable = true)\n",
      " |    |-- replyCount: long (nullable = true)\n",
      " |    |-- retweetCount: long (nullable = true)\n",
      " |    |-- retweetedTweet: string (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- sourceLabel: string (nullable = true)\n",
      " |    |-- sourceUrl: string (nullable = true)\n",
      " |    |-- tcooutlinks: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- created: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- descriptionUrls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- tcourl: string (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |-- favouritesCount: long (nullable = true)\n",
      " |    |    |-- followersCount: long (nullable = true)\n",
      " |    |    |-- friendsCount: long (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |-- listedCount: long (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- mediaCount: long (nullable = true)\n",
      " |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |-- statusesCount: long (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |    |-- verified: boolean (nullable = true)\n",
      " |-- renderedContent: string (nullable = true)\n",
      " |-- replyCount: long (nullable = true)\n",
      " |-- retweetCount: long (nullable = true)\n",
      " |-- retweetedTweet: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- sourceLabel: string (nullable = true)\n",
      " |-- sourceUrl: string (nullable = true)\n",
      " |-- tcooutlinks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- created: string (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- descriptionUrls: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- tcourl: string (nullable = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- displayname: string (nullable = true)\n",
      " |    |-- favouritesCount: long (nullable = true)\n",
      " |    |-- followersCount: long (nullable = true)\n",
      " |    |-- friendsCount: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- linkTcourl: string (nullable = true)\n",
      " |    |-- linkUrl: string (nullable = true)\n",
      " |    |-- listedCount: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- mediaCount: long (nullable = true)\n",
      " |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- rawDescription: string (nullable = true)\n",
      " |    |-- statusesCount: long (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------------------+----+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+----------+------------+--------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             content|     conversationId|                date|                 id|lang|likeCount|               media|      mentionedUsers|            outlinks|quoteCount|         quotedTweet|     renderedContent|replyCount|retweetCount|retweetedTweet|              source|        sourceLabel|           sourceUrl|         tcooutlinks|                 url|                user|\n",
      "+--------------------+-------------------+--------------------+-------------------+----+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+----------+------------+--------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The world progres...|1364506249291784198|2021-02-24T09:23:...|1364506249291784198|  en|        0|                NULL|[{NULL, NULL, NUL...|[https://twitter....|         0|{This is what the...|The world progres...|         0|           0|          NULL|<a href=\"http://t...| Twitter for iPhone|http://twitter.co...|[https://t.co/es3...|https://twitter.c...|{2009-06-06T07:50...|\n",
      "|#FarmersProtest \\...|1364506237451313155|2021-02-24T09:23:...|1364506237451313155|  en|        0|[{139.934, NULL, ...|[{NULL, NULL, NUL...|                  []|         0|                NULL|#FarmersProtest \\...|         0|           0|          NULL|<a href=\"http://t...|Twitter for Android|http://twitter.co...|                  []|https://twitter.c...|{2021-01-29T09:58...|\n",
      "|ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾ...|1364506195453767680|2021-02-24T09:23:...|1364506195453767680|  pa|        0|                NULL|                NULL|                  []|         0|                NULL|ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾ...|         0|           0|          NULL|<a href=\"http://t...|Twitter for Android|http://twitter.co...|                  []|https://twitter.c...|{2012-01-27T17:30...|\n",
      "|@ReallySwara @roh...|1364350947099484160|2021-02-24T09:23:...|1364506167226032128|  en|        0|[{46.375, NULL, N...|[{NULL, NULL, NUL...|[https://youtu.be...|         0|                NULL|@ReallySwara @roh...|         0|           0|          NULL|<a href=\"https://...|    Twitter Web App|https://mobile.tw...|[https://t.co/wBP...|https://twitter.c...|{2010-04-28T03:12...|\n",
      "|#KisanEktaMorcha ...|1364506144002088963|2021-02-24T09:23:...|1364506144002088963| und|        0|[{NULL, https://p...|                NULL|                  []|         0|                NULL|#KisanEktaMorcha ...|         0|           0|          NULL|<a href=\"http://t...| Twitter for iPhone|http://twitter.co...|                  []|https://twitter.c...|{2021-02-08T14:35...|\n",
      "+--------------------+-------------------+--------------------+-------------------+----+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+----------+------------+--------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Temp View for Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"farmers_protest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring columns to be used in data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             content|      mentionedUsers|\n",
      "+--------------------+--------------------+\n",
      "|The world progres...|[{NULL, NULL, NUL...|\n",
      "|#FarmersProtest \\...|[{NULL, NULL, NUL...|\n",
      "|ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾ...|                NULL|\n",
      "|@ReallySwara @roh...|[{NULL, NULL, NUL...|\n",
      "|#KisanEktaMorcha ...|                NULL|\n",
      "|Jai jwaan jai kis...|                NULL|\n",
      "|     #FarmersProtest|                NULL|\n",
      "|#ModiDontSellFarm...|                NULL|\n",
      "|@mandeeppunia1 wa...|[{NULL, NULL, NUL...|\n",
      "|#FarmersProtest h...|                NULL|\n",
      "|கோதுமைப் பயிர்களை...|                NULL|\n",
      "|@mandeeppunia1 wa...|[{NULL, NULL, NUL...|\n",
      "|Another farmer, M...|                NULL|\n",
      "|Jai kissan #Farme...|                NULL|\n",
      "|#FarmersProtest h...|                NULL|\n",
      "|ਸਰਕਾਰੇ ਨੀ ਤੇਰੇ ਕੰ...|                NULL|\n",
      "|@akshaykumar Hi c...|[{NULL, NULL, NUL...|\n",
      "|#ModiDontSellFarm...|                NULL|\n",
      "|@taapsee watch fu...|[{NULL, NULL, NUL...|\n",
      "|#FarmersProtest h...|                NULL|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        content,\n",
    "        mentionedUsers\n",
    "    FROM\n",
    "        farmers_protest\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding data usage for challenge\n",
    "\n",
    "q1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días.\n",
    "\n",
    "**Columns: id, date, user.username**\n",
    "\n",
    "\n",
    "q2. Los top 10 emojis más usados con su respectivo conteo.\n",
    "\n",
    "**Columns: id, content**\n",
    "\n",
    "\n",
    "q3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos. \n",
    "\n",
    "The \"mentionedUsers\" at the main tweet level appear to be filled with null values, necessitating the transformation of the content to retrieve the users.\n",
    "\n",
    "**Columns: id, content, user.username**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+---------------+\n",
      "|                 id|                date|             content|       username|\n",
      "+-------------------+--------------------+--------------------+---------------+\n",
      "|1364506249291784198|2021-02-24T09:23:...|The world progres...|ArjunSinghPanam|\n",
      "|1364506237451313155|2021-02-24T09:23:...|#FarmersProtest \\...|     PrdeepNain|\n",
      "|1364506195453767680|2021-02-24T09:23:...|ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾ...| parmarmaninder|\n",
      "|1364506167226032128|2021-02-24T09:23:...|@ReallySwara @roh...|  anmoldhaliwal|\n",
      "|1364506144002088963|2021-02-24T09:23:...|#KisanEktaMorcha ...|     KotiaPreet|\n",
      "|1364506120497360896|2021-02-24T09:23:...|Jai jwaan jai kis...|      babli_708|\n",
      "|1364506076272496640|2021-02-24T09:22:...|     #FarmersProtest|Varinde17354019|\n",
      "|1364505995859423234|2021-02-24T09:22:...|#ModiDontSellFarm...|    BitnamSingh|\n",
      "|1364505991887347714|2021-02-24T09:22:...|@mandeeppunia1 wa...|  anmoldhaliwal|\n",
      "|1364505896576053248|2021-02-24T09:22:...|#FarmersProtest h...|      SatThiara|\n",
      "|1364505892612268032|2021-02-24T09:22:...|கோதுமைப் பயிர்களை...| PasumaiVikatan|\n",
      "|1364505813834989568|2021-02-24T09:21:...|@mandeeppunia1 wa...|  anmoldhaliwal|\n",
      "|1364505749359976448|2021-02-24T09:21:...|Another farmer, M...| ShariaActivist|\n",
      "|1364505737695739906|2021-02-24T09:21:...|Jai kissan #Farme...|      babli_708|\n",
      "|1364505706804744192|2021-02-24T09:21:...|#FarmersProtest h...|       Dallehal|\n",
      "|1364505702715154439|2021-02-24T09:21:...|ਸਰਕਾਰੇ ਨੀ ਤੇਰੇ ਕੰ...|KaurAma57668156|\n",
      "|1364505676375076867|2021-02-24T09:21:...|@akshaykumar Hi c...|KaurDosanjh1979|\n",
      "|1364505591641735170|2021-02-24T09:20:...|#ModiDontSellFarm...|ArjunSinghPanam|\n",
      "|1364505511073300481|2021-02-24T09:20:...|@taapsee watch fu...|  anmoldhaliwal|\n",
      "|1364505462419447810|2021-02-24T09:20:...|#FarmersProtest h...|    BitnamSingh|\n",
      "+-------------------+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        date,\n",
    "        content,\n",
    "        user.username\n",
    "        \n",
    "    FROM\n",
    "        farmers_protest\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking null values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+--------+\n",
      "| id|date|content|username|\n",
      "+---+----+-------+--------+\n",
      "|  0|   0|      0|       0|\n",
      "+---+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_counts = result.select([sum(when(result[col].isNull(), 1).otherwise(0)).alias(col) for col in result.columns])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values are not presented in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking duplicated data by id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = result.groupBy(\"id\").count()\n",
    "duplications = duplicate_count.filter(duplicate_count[\"count\"] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "duplications.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplications by id in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each question, I will utilize memory usage and execution time measurements.\n",
    "\n",
    "Memory_profiler: I will analyze each step of my code to understand possible refinements of memory usage during each stage of my data processing. To achieve this, I'll use the memory_profiler library to profile memory consumption at various points in my code. This will provide insights into memory-intensive operations that can be optimized.\n",
    "\n",
    "Time: To measure the execution time of my data processing, I'll use the datetime differences approach. In the Jupyter notebook, I will record the start and end times before and after the code execution and calculate the time difference. This will help me assess the performance of my code and identify areas that may benefit from time optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General observations\n",
    "\n",
    "\n",
    "It is important to analyze the volume of your data to define what kind of memory usage you want to define in your SparkSession:\n",
    "\n",
    "```\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FarmersProtestTweets\") \\\n",
    "    .config(\"spark.executor.memory\", \"MEMORY ALOCATED\") \\\n",
    "    .config(\"spark.driver.memory\", \"MEMORY ALOCATED\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "For this specific case, after the data transformation, we end up with a dataset of only 27 MB. In such cases, I prefer to let my SparkSession use the default memory allocation settings. However, for different scenarios, you can choose to persist your data on disk. This may slightly increase data processing time but reduce the demand on driver memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime to analyze execution time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_DATA_PATH = \"../data/staging/farmers-protest-tweets-staging.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession_default = SparkSession.builder\\\n",
    "    .appName(\"FarmersProtestTweets\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession_optimization = SparkSession.builder \\\n",
    "    .appName(\"FarmersProtestTweetsOptmization\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 12:14:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    17    102.2 MiB    102.2 MiB           1   @profile\n",
      "    18                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    19                                             \n",
      "    20    102.3 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweets\").getOrCreate()\n",
      "    21                                             \n",
      "    22    102.3 MiB      0.0 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    23                                             \n",
      "    24                                             #Top 10 dates with more content\n",
      "    25    102.3 MiB      0.0 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    26    102.3 MiB      0.0 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    27                                         \n",
      "    28                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    29    102.3 MiB      0.0 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    30                                         \n",
      "    31                                             #Counting Users posts on top 10 dates\n",
      "    32    102.3 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    33                                         \n",
      "    34                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    35                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    36    102.3 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    37                                         \n",
      "    38                                             #creating rank based in row_number ordering\n",
      "    39    102.3 MiB      0.0 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    40                                         \n",
      "    41                                             #getting the Rank1 Username for each date\n",
      "    42    102.3 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    43                                         \n",
      "    44                                             #ordering by content count by date\n",
      "    45    102.3 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    46                                         \n",
      "    47                                             # Collect dataframe results\n",
      "    48    102.3 MiB      0.0 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    49                                         \n",
      "    50                                             \n",
      "    51                                         \n",
      "    52                                             #Creating result list of tupples\n",
      "    53    102.3 MiB      0.0 MiB           1       result = []\n",
      "    54    102.3 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    55    102.3 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    56                                         \n",
      "    57                                         \n",
      "    58    102.3 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "result_q1_memory = q1_memory(STAGING_DATA_PATH)\n",
    "end_time = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:04.781052\n"
     ]
    }
   ],
   "source": [
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 12:14:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/11/03 12:14:54 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    18    102.3 MiB    102.3 MiB           1   @profile\n",
      "    19                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    20                                                 \n",
      "    21    102.3 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweetsOptmization\").getOrCreate()\n",
      "    22                                         \n",
      "    23                                         \n",
      "    24    102.3 MiB      0.0 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    25    102.3 MiB      0.0 MiB           1       df.persist(StorageLevel.MEMORY_ONLY)\n",
      "    26                                         \n",
      "    27                                             #Top 10 dates with more content\n",
      "    28    102.3 MiB      0.0 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    29    102.3 MiB      0.0 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    30                                         \n",
      "    31                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    32    102.3 MiB      0.0 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    33                                         \n",
      "    34                                             #Counting Users posts on top 10 dates\n",
      "    35    102.3 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    36                                         \n",
      "    37                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    38                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    39    102.3 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    40                                         \n",
      "    41                                             #creating rank based in row_number ordering\n",
      "    42    102.3 MiB      0.0 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    43                                         \n",
      "    44                                             #getting the Rank1 Username for each date\n",
      "    45    102.3 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    46                                         \n",
      "    47                                             #ordering by content count by date\n",
      "    48    102.3 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    49                                         \n",
      "    50                                             # Collect dataframe results\n",
      "    51    102.3 MiB      0.0 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    52                                         \n",
      "    53                                         \n",
      "    54                                             #Creating result list of tupples\n",
      "    55    102.3 MiB      0.0 MiB           1       result = []\n",
      "    56    102.3 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    57    102.3 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    58                                         \n",
      "    59                                         \n",
      "    60    102.3 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "result_q1_time = q1_time(STAGING_DATA_PATH)\n",
    "end_time = datetime.now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:03.785077\n"
     ]
    }
   ],
   "source": [
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The memory profiler cannot access JVM memory usage. Consequently, the memory usage will appear stable in the report.\n",
    "2. The data volume of the staging data (27 MB) is inadequate for a comprehensive performance evaluation in PySpark. To address this, I will create two additional mocked datasets with larger data volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running my functions with a 2.2 GB data volume, it was still not possible to perform a thorough bottleneck analysis for my code. The following notebook steps will be analyzed using a 22 GB mocked dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_DATA_PATH = \"../data/staging/farmers-protest-tweets-staging.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/03 13:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    17    102.6 MiB    102.6 MiB           1   @profile\n",
      "    18                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    19                                             \n",
      "    20    103.9 MiB      1.3 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweets\").getOrCreate()\n",
      "    21                                             \n",
      "    22    103.9 MiB      0.0 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    23                                             \n",
      "    24                                             #Top 10 dates with more content\n",
      "    25    104.0 MiB      0.0 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    26    104.0 MiB      0.0 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    27                                         \n",
      "    28                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    29    104.0 MiB      0.0 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    30                                         \n",
      "    31                                             #Counting Users posts on top 10 dates\n",
      "    32    104.0 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    33                                         \n",
      "    34                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    35                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    36    104.0 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    37                                         \n",
      "    38                                             #creating rank based in row_number ordering\n",
      "    39    104.0 MiB      0.0 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    40                                         \n",
      "    41                                             #getting the Rank1 Username for each date\n",
      "    42    104.0 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    43                                         \n",
      "    44                                             #ordering by content count by date\n",
      "    45    104.0 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    46                                         \n",
      "    47                                             # Collect dataframe results\n",
      "    48    104.5 MiB      0.6 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    49                                         \n",
      "    50                                             \n",
      "    51                                         \n",
      "    52                                             #Creating result list of tupples\n",
      "    53    104.5 MiB      0.0 MiB           1       result = []\n",
      "    54    104.5 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    55    104.5 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    56                                         \n",
      "    57                                         \n",
      "    58    104.5 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_q1 = q1_memory(STAGING_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(result_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q2_memory import q2_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_q2 = q2_memory(STAGING_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5016),\n",
       " ('😂', 3034),\n",
       " ('🚜', 2959),\n",
       " ('🌾', 2155),\n",
       " ('🇮🇳', 2074),\n",
       " ('❤', 1766),\n",
       " ('🤣', 1646),\n",
       " ('✊', 1615),\n",
       " ('🙏🏻', 1315),\n",
       " ('💚', 1026)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q3_memory import q3_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_q3 = q3_memory(STAGING_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@narendramodi', 1124),\n",
       " ('@Kisanektamorcha', 976),\n",
       " ('@RakeshTikaitBKU', 761),\n",
       " ('@PMOIndia', 664),\n",
       " ('@RaviSinghKA', 580),\n",
       " ('@YouTube', 567),\n",
       " ('@Tractor2twitr', 489),\n",
       " ('@RahulGandhi', 456),\n",
       " ('@DelhiPolice', 360),\n",
       " ('@rihanna', 355)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Time optmization in PySpark for Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, Q1 was selected for a detailed examination in the Spark Web UI.\n",
    "\n",
    "Steps to be followed:\n",
    "\n",
    "1. Using data/test/mock_volume_data.py, I will generate a 20 GB dataset.\n",
    "2. Using the q1_memory function, I will analyze potential bottlenecks in processing via the Spark Web UI. If possible, I will implement caching in q1_time to improve processing time.\n",
    "3. Subsequently, I will compare differences in Spark job steps regarding memory usage and execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGER_DATA_DIR = \"../data/test/test_volume_data20gb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    17    105.3 MiB    105.3 MiB           1   @profile\n",
      "    18                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    19                                             \n",
      "    20    105.3 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweets\").getOrCreate()\n",
      "    21                                             \n",
      "    22    105.3 MiB      0.0 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    23                                             \n",
      "    24                                             #Top 10 dates with more content\n",
      "    25    105.3 MiB      0.0 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    26    105.3 MiB      0.0 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    27                                         \n",
      "    28                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    29    105.3 MiB      0.0 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    30                                         \n",
      "    31                                             #Counting Users posts on top 10 dates\n",
      "    32    105.3 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    33                                         \n",
      "    34                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    35                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    36    105.3 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    37                                         \n",
      "    38                                             #creating rank based in row_number ordering\n",
      "    39    105.3 MiB      0.0 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    40                                         \n",
      "    41                                             #getting the Rank1 Username for each date\n",
      "    42    105.3 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    43                                         \n",
      "    44                                             #ordering by content count by date\n",
      "    45    105.3 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    46                                         \n",
      "    47                                             # Collect dataframe results\n",
      "    48    105.3 MiB      0.0 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    49                                         \n",
      "    50                                             \n",
      "    51                                         \n",
      "    52                                             #Creating result list of tupples\n",
      "    53    105.3 MiB      0.0 MiB           1       result = []\n",
      "    54    105.3 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    55    105.3 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    56                                         \n",
      "    57                                         \n",
      "    58    105.3 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = q1_memory(LARGER_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the memory profiler is unable to access memory usage data for Spark jobs. To analyze the execution of my function, I accessed the Spark Web UI and retrieved information about my function's execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Spark_job_id |  duration  | peak Execution Memory  | Peak JVM Memory  | Job step description                   |\n",
    "|---------------|------------|------------------------|------------------|----------------------------------------|\n",
    "|             0 |  3.6 min   |       256kb            |     289 mb       |  data read and load                    |     \n",
    "|             1 |  62 ms     |       2.2 mb           |     162 mb       |  count by date                         |     \n",
    "|             2 |  3.8 min   |       4.3 mb           |     286 mb       |  filtering original df                 |           \n",
    "|             3 |  0.2 s     |       5.5 mb           |     0 mb         |  count by user in filtered_df          |        \n",
    "|             4 |  0.2 s     |       2.1 mb           |     0 mb         |  create analytical window              |     \n",
    "|             5 |  41 ms     |       2.1 mb           |     0 mb         |  filtering users by row_number rank    |   \n",
    "|             6 |  28 ms     |       2.1 mb           |     0 mb         |  classify to generate result           |\n",
    "\n",
    "Analyzing the stages in a Spark job is crucial for understanding performance and resource usage. The provided table offers valuable insights into job execution. We observe that stages 0 and 2 stand out in terms of duration, indicating that data reading and filtering of the original DataFrame are more time-consuming operations. The variation in peak execution memory and peak JVM memory underscores the complexity of operations across different stages. Step descriptions provide detailed information on what each stage accomplishes. Runtime varies widely, highlighting the diversity of operations. This analysis is essential for optimizing Spark job performance and allocating resources effectively.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary conclusions for q1_memory execution\n",
    "\n",
    "\n",
    "By using a non-partitioned CSV file as the data source, it was not possible to parallelize the reading process in PySpark, as evidenced in Spark job 0. The large, unpartitioned file led to a single-threaded reading process, resulting in a longer read time.\n",
    "\n",
    "The filtering step to select only the necessary data significantly improved the subsequent steps by reducing memory usage. This optimization allowed for more efficient processing and helped to overcome the limitations posed by the initial non-partitioned file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 14:38:12 ERROR TaskContextImpl: Error in TaskCompletionListener 1) / 1]\n",
      "org.apache.spark.SparkException: Block broadcast_3 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:390)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:1309)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1(TorrentBroadcast.scala:319)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1$adapted(TorrentBroadcast.scala:319)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:132)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:172)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "23/11/03 14:38:12 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2): Java heap space\n",
      "23/11/03 14:38:26 WARN MemoryStore: Not enough space to cache rdd_3_0 in memory! (computed 309.9 MiB so far)\n",
      "23/11/03 14:38:26 WARN BlockManager: Block rdd_3_0 could not be removed as it was not found on disk or in memory\n",
      "23/11/03 14:38:26 WARN BlockManager: Putting block rdd_3_0 failed\n",
      "23/11/03 14:38:41 WARN MemoryStore: Not enough space to cache rdd_3_0 in memory! (computed 309.9 MiB so far)\n",
      "23/11/03 14:38:41 WARN BlockManager: Block rdd_3_0 could not be removed as it was not found on disk or in memory\n",
      "23/11/03 14:38:41 WARN BlockManager: Putting block rdd_3_0 failed\n",
      "23/11/03 14:38:56 WARN MemoryStore: Not enough space to cache rdd_3_0 in memory! (computed 309.9 MiB so far)\n",
      "23/11/03 14:38:56 WARN BlockManager: Block rdd_3_0 could not be removed as it was not found on disk or in memory\n",
      "23/11/03 14:38:56 WARN BlockManager: Putting block rdd_3_0 failed\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/k7/ssh167fx77dc8g8s0ks7cljr0000gn/T/ipykernel_1543/4170174916.py\", line 1, in <module>\n",
      "    result = q1_time(LARGER_DATA_DIR)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/memory_profiler.py\", line 1188, in wrapper\n",
      "    val = prof(func)(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/memory_profiler.py\", line 761, in f\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_time.py\", line 51, in q1_time\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/dataframe.py\", line 1257, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "result = q1_time(LARGER_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession_default.stop()\n",
    "sparkSession_optimization.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
