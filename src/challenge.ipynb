{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/raw/farmers-protest-tweets-2021-2-4.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession builder and file read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/03 12:00:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/03 12:00:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FarmersProtestTweets\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe print schema and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- conversationId: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- likeCount: long (nullable = true)\n",
      " |-- media: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- duration: double (nullable = true)\n",
      " |    |    |-- fullUrl: string (nullable = true)\n",
      " |    |    |-- previewUrl: string (nullable = true)\n",
      " |    |    |-- thumbnailUrl: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- variants: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |-- contentType: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |-- mentionedUsers: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- created: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- descriptionUrls: string (nullable = true)\n",
      " |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |-- favouritesCount: string (nullable = true)\n",
      " |    |    |-- followersCount: string (nullable = true)\n",
      " |    |    |-- friendsCount: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |-- listedCount: string (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- mediaCount: string (nullable = true)\n",
      " |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |-- protected: string (nullable = true)\n",
      " |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |-- statusesCount: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |    |-- verified: string (nullable = true)\n",
      " |-- outlinks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- quoteCount: long (nullable = true)\n",
      " |-- quotedTweet: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- conversationId: long (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- likeCount: long (nullable = true)\n",
      " |    |-- media: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- duration: double (nullable = true)\n",
      " |    |    |    |-- fullUrl: string (nullable = true)\n",
      " |    |    |    |-- previewUrl: string (nullable = true)\n",
      " |    |    |    |-- thumbnailUrl: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- variants: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |    |-- contentType: string (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- mentionedUsers: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- descriptionUrls: string (nullable = true)\n",
      " |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |-- favouritesCount: string (nullable = true)\n",
      " |    |    |    |-- followersCount: string (nullable = true)\n",
      " |    |    |    |-- friendsCount: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |-- listedCount: string (nullable = true)\n",
      " |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |-- mediaCount: string (nullable = true)\n",
      " |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |-- protected: string (nullable = true)\n",
      " |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |-- statusesCount: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |-- verified: string (nullable = true)\n",
      " |    |-- outlinks: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- quoteCount: long (nullable = true)\n",
      " |    |-- quotedTweet: struct (nullable = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |    |    |-- conversationId: long (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- likeCount: long (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- duration: double (nullable = true)\n",
      " |    |    |    |    |-- fullUrl: string (nullable = true)\n",
      " |    |    |    |    |-- previewUrl: string (nullable = true)\n",
      " |    |    |    |    |-- thumbnailUrl: string (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- variants: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- contentType: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- mentionedUsers: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- descriptionUrls: string (nullable = true)\n",
      " |    |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |    |-- favouritesCount: string (nullable = true)\n",
      " |    |    |    |    |-- followersCount: string (nullable = true)\n",
      " |    |    |    |    |-- friendsCount: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |    |-- listedCount: string (nullable = true)\n",
      " |    |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |    |-- mediaCount: string (nullable = true)\n",
      " |    |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |    |-- protected: string (nullable = true)\n",
      " |    |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |    |-- statusesCount: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |    |-- verified: string (nullable = true)\n",
      " |    |    |-- outlinks: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- quoteCount: long (nullable = true)\n",
      " |    |    |-- quotedTweet: struct (nullable = true)\n",
      " |    |    |    |-- content: string (nullable = true)\n",
      " |    |    |    |-- conversationId: long (nullable = true)\n",
      " |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- lang: string (nullable = true)\n",
      " |    |    |    |-- likeCount: long (nullable = true)\n",
      " |    |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- duration: double (nullable = true)\n",
      " |    |    |    |    |    |-- fullUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- previewUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- thumbnailUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |    |-- variants: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- contentType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- mentionedUsers: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |    |-- descriptionUrls: string (nullable = true)\n",
      " |    |    |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |    |    |-- favouritesCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- followersCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- friendsCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- listedCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |    |    |-- mediaCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |    |    |-- protected: string (nullable = true)\n",
      " |    |    |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |    |    |-- statusesCount: string (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |    |    |-- verified: string (nullable = true)\n",
      " |    |    |    |-- outlinks: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- quoteCount: long (nullable = true)\n",
      " |    |    |    |-- quotedTweet: string (nullable = true)\n",
      " |    |    |    |-- renderedContent: string (nullable = true)\n",
      " |    |    |    |-- replyCount: long (nullable = true)\n",
      " |    |    |    |-- retweetCount: long (nullable = true)\n",
      " |    |    |    |-- retweetedTweet: string (nullable = true)\n",
      " |    |    |    |-- source: string (nullable = true)\n",
      " |    |    |    |-- sourceLabel: string (nullable = true)\n",
      " |    |    |    |-- sourceUrl: string (nullable = true)\n",
      " |    |    |    |-- tcooutlinks: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- user: struct (nullable = true)\n",
      " |    |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- descriptionUrls: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |    |-- favouritesCount: long (nullable = true)\n",
      " |    |    |    |    |-- followersCount: long (nullable = true)\n",
      " |    |    |    |    |-- friendsCount: long (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |    |-- listedCount: long (nullable = true)\n",
      " |    |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |    |-- mediaCount: long (nullable = true)\n",
      " |    |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |    |-- statusesCount: long (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |    |-- verified: boolean (nullable = true)\n",
      " |    |    |-- renderedContent: string (nullable = true)\n",
      " |    |    |-- replyCount: long (nullable = true)\n",
      " |    |    |-- retweetCount: long (nullable = true)\n",
      " |    |    |-- retweetedTweet: string (nullable = true)\n",
      " |    |    |-- source: string (nullable = true)\n",
      " |    |    |-- sourceLabel: string (nullable = true)\n",
      " |    |    |-- sourceUrl: string (nullable = true)\n",
      " |    |    |-- tcooutlinks: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user: struct (nullable = true)\n",
      " |    |    |    |-- created: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- descriptionUrls: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |    |-- tcourl: string (nullable = true)\n",
      " |    |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |    |-- favouritesCount: long (nullable = true)\n",
      " |    |    |    |-- followersCount: long (nullable = true)\n",
      " |    |    |    |-- friendsCount: long (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |    |-- listedCount: long (nullable = true)\n",
      " |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |-- mediaCount: long (nullable = true)\n",
      " |    |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |    |-- statusesCount: long (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |-- verified: boolean (nullable = true)\n",
      " |    |-- renderedContent: string (nullable = true)\n",
      " |    |-- replyCount: long (nullable = true)\n",
      " |    |-- retweetCount: long (nullable = true)\n",
      " |    |-- retweetedTweet: string (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- sourceLabel: string (nullable = true)\n",
      " |    |-- sourceUrl: string (nullable = true)\n",
      " |    |-- tcooutlinks: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- created: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- descriptionUrls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- tcourl: string (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- displayname: string (nullable = true)\n",
      " |    |    |-- favouritesCount: long (nullable = true)\n",
      " |    |    |-- followersCount: long (nullable = true)\n",
      " |    |    |-- friendsCount: long (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- linkTcourl: string (nullable = true)\n",
      " |    |    |-- linkUrl: string (nullable = true)\n",
      " |    |    |-- listedCount: long (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- mediaCount: long (nullable = true)\n",
      " |    |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |-- rawDescription: string (nullable = true)\n",
      " |    |    |-- statusesCount: long (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |    |-- verified: boolean (nullable = true)\n",
      " |-- renderedContent: string (nullable = true)\n",
      " |-- replyCount: long (nullable = true)\n",
      " |-- retweetCount: long (nullable = true)\n",
      " |-- retweetedTweet: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- sourceLabel: string (nullable = true)\n",
      " |-- sourceUrl: string (nullable = true)\n",
      " |-- tcooutlinks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- created: string (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- descriptionUrls: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- tcourl: string (nullable = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- displayname: string (nullable = true)\n",
      " |    |-- favouritesCount: long (nullable = true)\n",
      " |    |-- followersCount: long (nullable = true)\n",
      " |    |-- friendsCount: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- linkTcourl: string (nullable = true)\n",
      " |    |-- linkUrl: string (nullable = true)\n",
      " |    |-- listedCount: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- mediaCount: long (nullable = true)\n",
      " |    |-- profileBannerUrl: string (nullable = true)\n",
      " |    |-- profileImageUrl: string (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- rawDescription: string (nullable = true)\n",
      " |    |-- statusesCount: long (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------------------+----+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+----------+------------+--------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             content|     conversationId|                date|                 id|lang|likeCount|               media|      mentionedUsers|            outlinks|quoteCount|         quotedTweet|     renderedContent|replyCount|retweetCount|retweetedTweet|              source|        sourceLabel|           sourceUrl|         tcooutlinks|                 url|                user|\n",
      "+--------------------+-------------------+--------------------+-------------------+----+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+----------+------------+--------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The world progres...|1364506249291784198|2021-02-24T09:23:...|1364506249291784198|  en|        0|                NULL|[{NULL, NULL, NUL...|[https://twitter....|         0|{This is what the...|The world progres...|         0|           0|          NULL|<a href=\"http://t...| Twitter for iPhone|http://twitter.co...|[https://t.co/es3...|https://twitter.c...|{2009-06-06T07:50...|\n",
      "|#FarmersProtest \\...|1364506237451313155|2021-02-24T09:23:...|1364506237451313155|  en|        0|[{139.934, NULL, ...|[{NULL, NULL, NUL...|                  []|         0|                NULL|#FarmersProtest \\...|         0|           0|          NULL|<a href=\"http://t...|Twitter for Android|http://twitter.co...|                  []|https://twitter.c...|{2021-01-29T09:58...|\n",
      "|ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾ...|1364506195453767680|2021-02-24T09:23:...|1364506195453767680|  pa|        0|                NULL|                NULL|                  []|         0|                NULL|ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾ...|         0|           0|          NULL|<a href=\"http://t...|Twitter for Android|http://twitter.co...|                  []|https://twitter.c...|{2012-01-27T17:30...|\n",
      "|@ReallySwara @roh...|1364350947099484160|2021-02-24T09:23:...|1364506167226032128|  en|        0|[{46.375, NULL, N...|[{NULL, NULL, NUL...|[https://youtu.be...|         0|                NULL|@ReallySwara @roh...|         0|           0|          NULL|<a href=\"https://...|    Twitter Web App|https://mobile.tw...|[https://t.co/wBP...|https://twitter.c...|{2010-04-28T03:12...|\n",
      "|#KisanEktaMorcha ...|1364506144002088963|2021-02-24T09:23:...|1364506144002088963| und|        0|[{NULL, https://p...|                NULL|                  []|         0|                NULL|#KisanEktaMorcha ...|         0|           0|          NULL|<a href=\"http://t...| Twitter for iPhone|http://twitter.co...|                  []|https://twitter.c...|{2021-02-08T14:35...|\n",
      "+--------------------+-------------------+--------------------+-------------------+----+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+----------+------------+--------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Temp View for Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"farmers_protest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring columns to be used in data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             content|      mentionedUsers|\n",
      "+--------------------+--------------------+\n",
      "|The world progres...|[{NULL, NULL, NUL...|\n",
      "|#FarmersProtest \\...|[{NULL, NULL, NUL...|\n",
      "|ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾ...|                NULL|\n",
      "|@ReallySwara @roh...|[{NULL, NULL, NUL...|\n",
      "|#KisanEktaMorcha ...|                NULL|\n",
      "|Jai jwaan jai kis...|                NULL|\n",
      "|     #FarmersProtest|                NULL|\n",
      "|#ModiDontSellFarm...|                NULL|\n",
      "|@mandeeppunia1 wa...|[{NULL, NULL, NUL...|\n",
      "|#FarmersProtest h...|                NULL|\n",
      "|கோதுமைப் பயிர்களை...|                NULL|\n",
      "|@mandeeppunia1 wa...|[{NULL, NULL, NUL...|\n",
      "|Another farmer, M...|                NULL|\n",
      "|Jai kissan #Farme...|                NULL|\n",
      "|#FarmersProtest h...|                NULL|\n",
      "|ਸਰਕਾਰੇ ਨੀ ਤੇਰੇ ਕੰ...|                NULL|\n",
      "|@akshaykumar Hi c...|[{NULL, NULL, NUL...|\n",
      "|#ModiDontSellFarm...|                NULL|\n",
      "|@taapsee watch fu...|[{NULL, NULL, NUL...|\n",
      "|#FarmersProtest h...|                NULL|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        content,\n",
    "        mentionedUsers\n",
    "    FROM\n",
    "        farmers_protest\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding data usage for challenge\n",
    "\n",
    "q1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días.\n",
    "\n",
    "**Columns: id, date, user.username**\n",
    "\n",
    "\n",
    "q2. Los top 10 emojis más usados con su respectivo conteo.\n",
    "\n",
    "**Columns: id, content**\n",
    "\n",
    "\n",
    "q3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos. \n",
    "\n",
    "The \"mentionedUsers\" at the main tweet level appear to be filled with null values, necessitating the transformation of the content to retrieve the users.\n",
    "\n",
    "**Columns: id, content, user.username**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+---------------+\n",
      "|                 id|                date|             content|       username|\n",
      "+-------------------+--------------------+--------------------+---------------+\n",
      "|1364506249291784198|2021-02-24T09:23:...|The world progres...|ArjunSinghPanam|\n",
      "|1364506237451313155|2021-02-24T09:23:...|#FarmersProtest \\...|     PrdeepNain|\n",
      "|1364506195453767680|2021-02-24T09:23:...|ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾ...| parmarmaninder|\n",
      "|1364506167226032128|2021-02-24T09:23:...|@ReallySwara @roh...|  anmoldhaliwal|\n",
      "|1364506144002088963|2021-02-24T09:23:...|#KisanEktaMorcha ...|     KotiaPreet|\n",
      "|1364506120497360896|2021-02-24T09:23:...|Jai jwaan jai kis...|      babli_708|\n",
      "|1364506076272496640|2021-02-24T09:22:...|     #FarmersProtest|Varinde17354019|\n",
      "|1364505995859423234|2021-02-24T09:22:...|#ModiDontSellFarm...|    BitnamSingh|\n",
      "|1364505991887347714|2021-02-24T09:22:...|@mandeeppunia1 wa...|  anmoldhaliwal|\n",
      "|1364505896576053248|2021-02-24T09:22:...|#FarmersProtest h...|      SatThiara|\n",
      "|1364505892612268032|2021-02-24T09:22:...|கோதுமைப் பயிர்களை...| PasumaiVikatan|\n",
      "|1364505813834989568|2021-02-24T09:21:...|@mandeeppunia1 wa...|  anmoldhaliwal|\n",
      "|1364505749359976448|2021-02-24T09:21:...|Another farmer, M...| ShariaActivist|\n",
      "|1364505737695739906|2021-02-24T09:21:...|Jai kissan #Farme...|      babli_708|\n",
      "|1364505706804744192|2021-02-24T09:21:...|#FarmersProtest h...|       Dallehal|\n",
      "|1364505702715154439|2021-02-24T09:21:...|ਸਰਕਾਰੇ ਨੀ ਤੇਰੇ ਕੰ...|KaurAma57668156|\n",
      "|1364505676375076867|2021-02-24T09:21:...|@akshaykumar Hi c...|KaurDosanjh1979|\n",
      "|1364505591641735170|2021-02-24T09:20:...|#ModiDontSellFarm...|ArjunSinghPanam|\n",
      "|1364505511073300481|2021-02-24T09:20:...|@taapsee watch fu...|  anmoldhaliwal|\n",
      "|1364505462419447810|2021-02-24T09:20:...|#FarmersProtest h...|    BitnamSingh|\n",
      "+-------------------+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        date,\n",
    "        content,\n",
    "        user.username\n",
    "        \n",
    "    FROM\n",
    "        farmers_protest\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking null values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+--------+\n",
      "| id|date|content|username|\n",
      "+---+----+-------+--------+\n",
      "|  0|   0|      0|       0|\n",
      "+---+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_counts = result.select([sum(when(result[col].isNull(), 1).otherwise(0)).alias(col) for col in result.columns])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values are not presented in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking duplicated data by id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = result.groupBy(\"id\").count()\n",
    "duplications = duplicate_count.filter(duplicate_count[\"count\"] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "duplications.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplications by id in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each question, I will utilize memory usage and execution time measurements.\n",
    "\n",
    "Memory_profiler: I will analyze each step of my code to understand possible refinements of memory usage during each stage of my data processing. To achieve this, I'll use the memory_profiler library to profile memory consumption at various points in my code. This will provide insights into memory-intensive operations that can be optimized.\n",
    "\n",
    "Time: To measure the execution time of my data processing, I'll use the datetime differences approach. In the Jupyter notebook, I will record the start and end times before and after the code execution and calculate the time difference. This will help me assess the performance of my code and identify areas that may benefit from time optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime memory customization for Spark\n",
    "\n",
    "\n",
    "spark.executor.memory: It controls how much memory each worker (executor) in a Spark cluster can use for processing tasks.\n",
    "\n",
    "spark.driver.memory: It specifies how much memory the main program (driver) of your Spark application can use for its operations.\n",
    "\n",
    "These settings are crucial for optimizing memory usage and performance, and you can adjust them based on your specific application needs and available cluster resources. Be cautious not to allocate too much or too little memory, as it can impact performance.\n",
    "\n",
    "```\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FarmersProtestTweets\") \\\n",
    "    .config(\"spark.executor.memory\", \"MEMORY ALOCATED\") \\\n",
    "    .config(\"spark.driver.memory\", \"MEMORY ALOCATED\") \\\n",
    "    .getOrCreate()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime to analyze execution time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_DATA_PATH = \"../data/staging/farmers-protest-tweets-staging.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating default SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/03 17:53:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sparkSession_default = SparkSession.builder\\\n",
    "    .appName(\"FarmersProtestTweets\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating custom memory allocation SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/03 18:19:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sparkSession_optimization = SparkSession.builder \\\n",
    "    .appName(\"FarmersProtestTweetsOptmization\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 17:56:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    17    105.8 MiB    105.8 MiB           1   @profile\n",
      "    18                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    19                                             \n",
      "    20    105.8 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweets\").getOrCreate()\n",
      "    21                                             \n",
      "    22    105.8 MiB      0.0 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    23                                             \n",
      "    24                                             #Top 10 dates with more content\n",
      "    25    105.8 MiB      0.0 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    26    105.8 MiB      0.0 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    27                                         \n",
      "    28                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    29    105.8 MiB      0.0 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    30                                         \n",
      "    31                                             #Counting Users posts on top 10 dates\n",
      "    32    105.8 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    33                                         \n",
      "    34                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    35                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    36    105.8 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    37                                         \n",
      "    38                                             #creating rank based in row_number ordering\n",
      "    39    105.8 MiB      0.0 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    40                                         \n",
      "    41                                             #getting the Rank1 Username for each date\n",
      "    42    105.8 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    43                                         \n",
      "    44                                             #ordering by content count by date\n",
      "    45    105.8 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    46                                         \n",
      "    47                                             # Collect dataframe results\n",
      "    48    105.8 MiB      0.0 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    49                                         \n",
      "    50                                             \n",
      "    51                                         \n",
      "    52                                             #Creating result list of tupples\n",
      "    53    105.8 MiB      0.0 MiB           1       result = []\n",
      "    54    105.8 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    55    105.8 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    56                                         \n",
      "    57                                         \n",
      "    58    105.8 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "result_q1_memory = q1_memory(STAGING_DATA_PATH)\n",
    "end_time = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:10.306601\n"
     ]
    }
   ],
   "source": [
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 17:57:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/11/03 17:57:07 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    18    105.8 MiB    105.8 MiB           1   @profile\n",
      "    19                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    20                                                 \n",
      "    21    105.8 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweetsOptmization\").getOrCreate()\n",
      "    22                                         \n",
      "    23                                         \n",
      "    24    105.8 MiB      0.0 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    25    105.8 MiB      0.0 MiB           1       df.persist(StorageLevel.MEMORY_AND_DISK)\n",
      "    26                                         \n",
      "    27                                             #Top 10 dates with more content\n",
      "    28    105.8 MiB      0.0 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    29    105.8 MiB      0.0 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    30                                         \n",
      "    31                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    32    105.8 MiB      0.0 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    33                                         \n",
      "    34                                             #Counting Users posts on top 10 dates\n",
      "    35    105.8 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    36                                         \n",
      "    37                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    38                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    39    105.8 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    40                                         \n",
      "    41                                             #creating rank based in row_number ordering\n",
      "    42    105.8 MiB      0.0 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    43                                         \n",
      "    44                                             #getting the Rank1 Username for each date\n",
      "    45    105.8 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    46                                         \n",
      "    47                                             #ordering by content count by date\n",
      "    48    105.8 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    49                                         \n",
      "    50                                             # Collect dataframe results\n",
      "    51    105.8 MiB      0.0 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    52                                         \n",
      "    53                                         \n",
      "    54                                             #Creating result list of tupples\n",
      "    55    105.8 MiB      0.0 MiB           1       result = []\n",
      "    56    105.8 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    57    105.8 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    58                                         \n",
      "    59                                         \n",
      "    60    105.8 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "result_q1_time = q1_time(STAGING_DATA_PATH)\n",
    "end_time = datetime.now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:01.092429\n"
     ]
    }
   ],
   "source": [
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The memory profiler cannot access JVM memory usage. Consequently, the memory usage will appear stable in the report.\n",
    "2. The data volume of the staging data (27 MB) is inadequate for a comprehensive performance evaluation in PySpark. To address this, I will create two additional mocked datasets with larger data volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running my functions with a 2.2 GB data volume, it was still not possible to perform a thorough bottleneck analysis for my code. The following notebook steps will be analyzed using a 22 GB mocked dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Time optmization in PySpark for Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, Q1 was selected for a detailed examination in the Spark Web UI.\n",
    "\n",
    "Steps to be followed:\n",
    "\n",
    "1. Using data/test/mock_volume_data.py, I will generate a 20 GB dataset.\n",
    "2. Using the q1_memory function, I will analyze potential bottlenecks in processing via the Spark Web UI. If possible, I will implement caching in q1_time to improve processing time.\n",
    "3. Subsequently, I will compare differences in Spark job steps regarding memory usage and execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGER_DATA_DIR = \"../data/test/test_volume_data20gb.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running q1_memory function with large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 17:57:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    17    105.8 MiB    105.8 MiB           1   @profile\n",
      "    18                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    19                                             \n",
      "    20    105.8 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweets\").getOrCreate()\n",
      "    21                                             \n",
      "    22    105.8 MiB      0.0 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    23                                             \n",
      "    24                                             #Top 10 dates with more content\n",
      "    25    105.8 MiB      0.0 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    26    105.8 MiB      0.0 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    27                                         \n",
      "    28                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    29    105.8 MiB      0.0 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    30                                         \n",
      "    31                                             #Counting Users posts on top 10 dates\n",
      "    32    105.8 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    33                                         \n",
      "    34                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    35                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    36    105.8 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    37                                         \n",
      "    38                                             #creating rank based in row_number ordering\n",
      "    39    105.8 MiB      0.0 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    40                                         \n",
      "    41                                             #getting the Rank1 Username for each date\n",
      "    42    105.8 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    43                                         \n",
      "    44                                             #ordering by content count by date\n",
      "    45    105.8 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    46                                         \n",
      "    47                                             # Collect dataframe results\n",
      "    48    104.9 MiB     -0.9 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    49                                         \n",
      "    50                                             \n",
      "    51                                         \n",
      "    52                                             #Creating result list of tupples\n",
      "    53    104.9 MiB      0.0 MiB           1       result = []\n",
      "    54    104.9 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    55    104.9 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    56                                         \n",
      "    57                                         \n",
      "    58    104.9 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = q1_memory(LARGER_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the memory profiler is unable to access memory usage data for Spark jobs. To analyze the execution of my function, I accessed the Spark Web UI and retrieved information about my function's execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Spark_job_id |  duration  | peak Execution Memory  | Peak JVM Memory  | Job step description                   |\n",
    "|---------------|------------|------------------------|------------------|----------------------------------------|\n",
    "|             0 |  3.6 min   |       256kb            |     289 mb       |  data read and load                    |     \n",
    "|             1 |  62 ms     |       2.2 mb           |     162 mb       |  count by date                         |     \n",
    "|             2 |  3.8 min   |       4.3 mb           |     286 mb       |  filtering original df                 |           \n",
    "|             3 |  0.2 s     |       5.5 mb           |     0 mb         |  count by user in filtered_df          |        \n",
    "|             4 |  0.2 s     |       2.1 mb           |     0 mb         |  create analytical window              |     \n",
    "|             5 |  41 ms     |       2.1 mb           |     0 mb         |  filtering users by row_number rank    |   \n",
    "|             6 |  28 ms     |       2.1 mb           |     0 mb         |  classify to generate result           |\n",
    "\n",
    "Analyzing the stages in a Spark job is crucial for understanding performance and resource usage. The provided table offers valuable insights into job execution. We observe that stages 0 and 2 stand out in terms of duration, indicating that data reading and filtering of the original DataFrame are more time-consuming operations. The variation in peak execution memory and peak JVM memory underscores the complexity of operations across different stages. Step descriptions provide detailed information on what each stage accomplishes. Runtime varies widely, highlighting the diversity of operations. This analysis is essential for optimizing Spark job performance and allocating resources effectively.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary conclusions for q1_memory execution\n",
    "\n",
    "\n",
    "By using a non-partitioned CSV file as the data source, it was not possible to parallelize the reading process in PySpark, as evidenced in Spark job 0. The large, unpartitioned file led to a single-threaded reading process, resulting in a longer read time.\n",
    "\n",
    "The filtering step to select only the necessary data significantly improved the subsequent steps by reducing memory usage. This optimization allowed for more efficient processing and helped to overcome the limitations posed by the initial non-partitioned file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Future improvements\n",
    "\n",
    "In real-world scenarios, if a file needs to be read more than once, it's a good practice to use a more performant file format like Parquet. \n",
    "\n",
    "Another technique to consider for such scenarios is repartitioning your data to improve the distribution of the reading process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running q1_time with larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import q1_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen optimization technique for q1_time was the creation of a persistence block in Memory and Disk. In the first execution, the optimized function will persist the data after the initial read, allowing for faster future accesses to this dataset.\n",
    "\n",
    "In practice, the concept of a \"slow start\" is implemented. The first execution of the optimization function will be more resource-intensive, but subsequent accesses to it will be faster.\n",
    "\n",
    "As a result, the accesses made by Q2 and Q3 to the cached dataset will be faster.\n",
    "\n",
    "To perform this case analysis, I will execute the q1_time function repeatedly to apply test and control analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 18:22:48 WARN MemoryStore: Not enough space to cache rdd_3_0 in memory! (computed 3.7 GiB so far)\n",
      "23/11/03 18:22:48 WARN BlockManager: Persisting block rdd_3_0 to disk instead.\n",
      "23/11/03 18:28:12 WARN BlockManager: Block rdd_3_0 already exists on this machine; not re-adding it\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    18     98.1 MiB     98.1 MiB           1   @profile\n",
      "    19                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    20                                                 \n",
      "    21     98.1 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweetsOptmization\").getOrCreate()\n",
      "    22                                         \n",
      "    23                                         \n",
      "    24     98.1 MiB      0.0 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    25     98.1 MiB      0.0 MiB           1       df.persist(StorageLevel.MEMORY_AND_DISK)\n",
      "    26                                         \n",
      "    27                                             #Top 10 dates with more content\n",
      "    28     98.1 MiB      0.0 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    29     98.1 MiB      0.0 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    30                                         \n",
      "    31                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    32     98.1 MiB      0.0 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    33                                         \n",
      "    34                                             #Counting Users posts on top 10 dates\n",
      "    35     98.2 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    36                                         \n",
      "    37                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    38                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    39     98.2 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    40                                         \n",
      "    41                                             #creating rank based in row_number ordering\n",
      "    42     98.2 MiB      0.0 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    43                                         \n",
      "    44                                             #getting the Rank1 Username for each date\n",
      "    45     98.2 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    46                                         \n",
      "    47                                             #ordering by content count by date\n",
      "    48     98.2 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    49                                         \n",
      "    50                                             # Collect dataframe results\n",
      "    51     34.2 MiB    -64.0 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    52                                         \n",
      "    53                                         \n",
      "    54                                             #Creating result list of tupples\n",
      "    55     34.3 MiB      0.1 MiB           1       result = []\n",
      "    56     34.3 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    57     34.3 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    58                                         \n",
      "    59                                         \n",
      "    60     34.4 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = q1_time(LARGER_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Spark_job_id |  duration  | peak Execution Memory  | Peak JVM Memory  | Job step description                   |\n",
    "|---------------|------------|------------------------|------------------|----------------------------------------|\n",
    "|             0 |  7.5 min   |       0 mb             |     7 gb         |  data read and load and caching        |     \n",
    "|             1 |  0.2 ms    |       2.2 mb           |     162 mb       |  count by date                         |     \n",
    "|             2 |  32 s      |       256 kb           |     5.2 gb       |  filtering original df                 |   \n",
    "\n",
    "Other steps will be ommited for this analysis\n",
    "\n",
    "Total execution time 8 minutes and 48 seconds\n",
    "\n",
    "\n",
    "Upon reviewing this table, it's clear that the data read and load operations, including caching, posed a significant bottleneck during the initial execution, leading to a substantial increase in memory usage.\n",
    "\n",
    "However, once this bottleneck was addressed, the operation that previously had a notably long execution time experienced a remarkable 90% reduction in its execution time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 18:29:43 WARN CacheManager: Asked to cache already cached data.\n",
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/rtakeshi/Documents/Projetos/challenge-DE/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    18     41.5 MiB     41.5 MiB           1   @profile\n",
      "    19                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    20                                                 \n",
      "    21     41.9 MiB      0.4 MiB           1       spark = SparkSession.builder.appName(\"FarmersProtestTweetsOptmization\").getOrCreate()\n",
      "    22                                         \n",
      "    23                                         \n",
      "    24     42.5 MiB      0.6 MiB           1       df = spark.read.option('delimiter', '~').option('header', True).option('multiline', True).schema(STAGING_SCHEMA).csv(file_path)\n",
      "    25     42.6 MiB      0.1 MiB           1       df.persist(StorageLevel.MEMORY_AND_DISK)\n",
      "    26                                         \n",
      "    27                                             #Top 10 dates with more content\n",
      "    28     43.1 MiB      0.5 MiB           1       date_counts = df.groupBy('date').agg(count('content').alias('date_count'))\n",
      "    29     43.1 MiB      0.1 MiB           1       date_counts = date_counts.orderBy(col('date_count').desc()).limit(10)\n",
      "    30                                         \n",
      "    31                                             #Joined DF filtering only top 10 dates by inner joining\n",
      "    32     43.7 MiB      0.6 MiB           1       filtered_df = df.join(date_counts, 'date', 'inner')\n",
      "    33                                         \n",
      "    34                                             #Counting Users posts on top 10 dates\n",
      "    35     43.7 MiB      0.0 MiB           1       user_counts_by_date = filtered_df.groupBy('date', 'date_count', 'username').agg(count('content').alias('user_count'))\n",
      "    36                                         \n",
      "    37                                             #Creating a window analytical function to filter top 1 username in each date\n",
      "    38                                             #Edge case: if there is a tie, the username will follow alphabetical ordering\n",
      "    39     43.7 MiB      0.0 MiB           1       window_spec = Window.partitionBy('date', 'date_count').orderBy(col('user_count').desc(), col('username'))\n",
      "    40                                         \n",
      "    41                                             #creating rank based in row_number ordering\n",
      "    42     43.8 MiB      0.1 MiB           1       user_counts_by_date = user_counts_by_date.withColumn('rank', row_number().over(window_spec))\n",
      "    43                                         \n",
      "    44                                             #getting the Rank1 Username for each date\n",
      "    45     43.8 MiB      0.0 MiB           1       top_users_by_date = user_counts_by_date.filter(user_counts_by_date['rank'] == 1)\n",
      "    46                                         \n",
      "    47                                             #ordering by content count by date\n",
      "    48     43.8 MiB      0.0 MiB           1       top_users_by_date = top_users_by_date.select(['date', 'username']).orderBy(col('date_count').desc())\n",
      "    49                                         \n",
      "    50                                             # Collect dataframe results\n",
      "    51     44.4 MiB      0.6 MiB           1       result_collection = top_users_by_date.collect()\n",
      "    52                                         \n",
      "    53                                         \n",
      "    54                                             #Creating result list of tupples\n",
      "    55     44.4 MiB      0.0 MiB           1       result = []\n",
      "    56     44.4 MiB      0.0 MiB          11       for row in result_collection:\n",
      "    57     44.4 MiB      0.0 MiB          10           result.append((row['date'], row['username']))\n",
      "    58                                         \n",
      "    59                                         \n",
      "    60     44.4 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = q1_time(LARGER_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Spark_job_id |  duration  | peak Execution Memory  | Peak JVM Memory  | Job step description                   |\n",
    "|---------------|------------|------------------------|------------------|----------------------------------------|\n",
    "|            8 |  29 ms     |       16.2 m           |     0 b          |  data read and load and caching        |     \n",
    "|             9 |  46 ms     |       2.2 mb           |     16 mb        |  count by date                         |     \n",
    "|             10 |  37 s      |       256 kb           |     2 gb         |  filtering original df                 |   \n",
    "\n",
    "Other steps will be ommited for this analysis\n",
    "\n",
    "Total execution time 1 minute 7 seconds\n",
    "\n",
    "\n",
    "During the second execution, the time required for data reading and the memory usage were completely eliminated due to the previous caching operation performed on the root dataframe. This caching step significantly improved the efficiency of accessing the data.\n",
    "\n",
    "In the third step, despite the extensive filtering and data processing, the execution time remained consistent, highlighting the effectiveness and reliability of the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Closing my SparkSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession_default.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession_optimization.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis conclusion\n",
    "\n",
    "In conclusion, this analysis highlights the effectiveness of caching techniques in large-scale data processing environments for optimizing execution time. Through the examination of execution steps, it became evident what the primary bottleneck to address for solving the challenge.\n",
    "\n",
    "Assuming that the functions for questions Q2 and Q3 are executed within the same session, the results of the analysis can be extrapolated to the other two scenarios.\n",
    "\n",
    "It's important to note that, for Q2 the analysis of this solution would be somewhat different due to the impact that UDFs can have on the overall performance of Spark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_DATA_PATH = \"../data/staging/farmers-protest-tweets-staging.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "\n",
    "q1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_q1 = q1_memory(STAGING_DATA_PATH)\n",
    "print(result_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "\n",
    "q2. Los top 10 emojis más usados con su respectivo conteo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q2_memory import q2_memory\n",
    "\n",
    "result_q2 = q2_memory(STAGING_DATA_PATH)\n",
    "print(result_q2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "\n",
    "q3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q3_memory import q3_memory\n",
    "\n",
    "result_q3 = q3_memory(STAGING_DATA_PATH)\n",
    "print(result_q3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
